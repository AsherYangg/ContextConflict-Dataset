{
    "question": "Summarize the following evidence in 2-3 sentences.",
    "content": {
        "source_1": "I’m only commenting on the assignment quality. In Week 2, the brief says to “build a model-serving API with authentication,” but the rubric never mentions token expiration or basic input validation. The sample solution hardcodes secrets in config.py, and the provided tests just ping /health and a single happy-path POST; my teammate’s version without error handling still passed. Peer reviews mostly said “looks good” and checked the README format rather than functionality. Also, the dependency list pinned a library with breaking changes, and the instructions didn’t reflect that for about a week; the dataset link threw a 404 for two days.",
        "source_2": "As an overall experience, this capstone pulled together the earlier modules for me. I built a small churn-prediction service: cleaned a public dataset, trained a baseline model in scikit-learn, then wrapped it with Flask and Docker. The milestones nudged me from notebook experiments to a container I could run end to end. I recorded a 5‑minute demo and wrote a short tech report using their templates, which kept me organized. I also tried the optional visualization task by adding a simple Streamlit dashboard. Over three weeks of evenings, I ended up with a GitHub repo and a walkthrough video I’ve already shared in job applications.",
        "source_3": "Only speaking about difficulty: the core checkpoints felt light. The notebooks arrive with most scaffolding, and I changed maybe three functions to satisfy the grader. The REST endpoint file already includes the route and JSON parsing, so I basically wired in predict() and returned the response. I wrapped up the required pieces in a single weekend. The optional challenges—batch inference and basic monitoring—looked like they’d add more depth, but they don’t affect the score, so I left them for later.",
        "source_4": "From a whole-course perspective, the experience got bumpy. The submission site timed out twice while I uploaded a 120 MB image, and the portal flagged it as late even though I started before the cutoff. Support sent a canned response suggesting a different browser. Peer review took five days because the cohort felt quiet; I posted in the forum to get enough reviews. One reviewer docked points for a rubric item that isn’t in the checklist, and there’s no clear appeal path beyond leaving a comment. By the time grades finalized, I’d lost the momentum I had when building the project.",
        "source_5": "Focusing strictly on assignment structure: the checklists line up with the deliverables. Each milestone specifies what goes into the repo—data folder, training script, inference script, environment file—and the rubric mirrors those items one by one. The starter unit tests caught a regression when I refactored the preprocessing pipeline, and the example repo shows a consistent layout to imitate. The peer‑review guide asks for line references and reproducibility steps, which pushed me to tighten my README and seed my splits. The optional tasks extend it to a free-tier deployment and a short load test, which rounded out the project for me."
    }
}